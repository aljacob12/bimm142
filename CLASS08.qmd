---
title: "CLASS08"
author: "Alen Jacob"
format: html
---
## Preparing the data
```{r}
wisc.df <- read.csv("WisconsinCancer.csv",row.names=1)
head(wisc.df)
```
Removed the diagnosis colum and keep it in a separate vector for later
```{r}
diagnosis <-as.factor(wisc.df[,1])
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

## Exploratory data analysis

>Q1. How many observations are in this dataset?
```{r}
dim(wisc.data)
```


>Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```


>Q3. How many variables/features in the data are suffixed with _mean?
First find column names 
```{r}
colnames(wisc.data)
```

Next U need to ssearch within the column names for "_mean" pattern. the 'grep()' function might help

```{r}
inds <- grep("_mean",colnames(wisc.data))
length(inds)
```

>How many dimensions are un this dataset

```{r}
ncol(wisc.data)
```


## Principal Component Analysis

Performing PCA

We nee to firt scale the data so that 
big numbers do not skew the data 

```{r}
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp(wisc.data, scale=T)
summary(wisc.pr )
```

```{r}
round(apply(wisc.data,2,sd), 3)
```



>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27
    
>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 Pcs capture 72%

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 Pcs capture 91%



##Interpreting PCA results

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

We cannot determine much from this plot 


```{r}
plot( wisc.pr$x[,1], wisc.pr$x[,2], col= diagnosis)
```

```{r}
pc <- as.data.frame( wisc.pr$x)
pc$diagnosis <- diagnosis
library(ggplot2)
ggplot(pc)+
aes(PC1,PC2, col=diagnosis)+
geom_point()
```

#Variance explained

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var/ sum(pr.var)
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
#Examine the PC Loadings

```{r}
head(wisc.pr$rotation)
```




>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```
There is a complicated mix of variables that go together to make up PC1


>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr) 
```
Need at least PC5



##Hierarchical clustering

>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
19

>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
The cluster of 4 is the optimal cluster amounts for the data what we have 


```{r}
#scale the wisc.data using the 'scale()' function
data.scaled <- scale(wisc.data)
```

```{r}
#calculate the euclidean distance:
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```


```{r}
plot(wisc.hclust)
abline(h= 19,col="red", lty=2)
```
```{r}
wisc.hclust.cluster <- cutree(wisc.hclust, k=4)
```

```{r}
table(wisc.hclust.cluster,diagnosis)
```


Can cut this tree to yield 


#combine methods PCA and HCLUST
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2])

```

PCA resluts that is use 'wisc.pr$x' as input to 'hclust()'.

Try clusttering in 3 PCS, that is not PC1,2,3

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
plot(wisc.pr.hclust)
```

Lets  cut the tree into 2 groups/clusters
```{r}
grps<- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col= grps)
```
```{r}
table(grps, diagnosis)
```






>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

I liked to use "ward.D2" becausse it was eassier for me to visualize the data where as in the other graphs the data was clumped together and it was hard to visualize.

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?
```{r}
(179+333)/(nrow(wisc.data))
```
```
It does a pretty good job of separating out the 2 diagnoses
